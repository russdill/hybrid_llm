{
    "config": {
        "step": {
            "user": {
                "title": "Hybrid Local Voice Backend",
                "description": "Configure your local LLM connection."
            }
        },
        "abort": {
            "already_configured": "Device is already configured"
        }
    },
    "options": {
        "step": {
            "init": {
                "title": "Hybrid LLM Options",
                "data": {
                    "url": "Ollama API URL",
                    "model": "Model",
                    "prompt": "Instructions",
                    "num_ctx": "Context window size",
                    "max_history": "Max history messages",
                    "keep_alive": "Keep alive",
                    "llm_hass_api": "Control Home Assistant",
                    "think": "Think before responding",
                    "filler_model": "Filler Model",
                    "filler_prompt": "Filler Prompt",
                    "enable_native_intents": "Enable Native Intents",
                    "enable_fuzzy_matching": "Enable Fuzzy Matching for Intents",
                    "enable_tracer": "Enable Performance Tracing"
                },
                "data_description": {
                    "prompt": "Instruct how the LLM should respond. This can be a template.",
                    "num_ctx": "Maximum number of text tokens the model can process. Lower to reduce Ollama RAM, or increase for a large number of exposed entities.",
                    "keep_alive": "Duration in seconds for Ollama to keep model in memory. -1 = indefinite, 0 = never.",
                    "think": "If enabled, the LLM will think before responding. This can improve response quality but may increase latency.",
                    "filler_model": "Small, fast model used to generate a brief 'checking...' phrase while the main model processes.",
                    "filler_prompt": "Prompt template for generating filler phrases.",
                    "enable_native_intents": "If enabled, built-in intents (e.g. Turn On Light) are checked locally before using the LLM.",
                    "enable_fuzzy_matching": "Fuzzy matching is more likely to make mistakes that the LLM could better handle.",
                    "enable_tracer": "If enabled, detailed performance timelines (JSON) will be saved to /config/traces/."
                }
            },
            "download": {
                "title": "Downloading Model",
                "description": "Please wait while the model is downloaded, which may take a very long time. Check your Ollama server logs for more details."
            }
        },
        "abort": {
            "download_failed": "Model download failed. Check Home Assistant logs."
        },
        "progress": {
            "download": "Please wait while the model is downloaded, which may take a very long time. Check your Ollama server logs for more details."
        }
    }
}